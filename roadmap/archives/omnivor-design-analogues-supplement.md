# OmniVor â€” Design Analogues Deep Dive (Supplement)

> Focused analysis on Audience Comparison, Action Queue, and HCP Explorer based on current UI screenshots and Will's direction on specific analogue categories.

---

## Grading Key

| Dimension | Scale | Meaning |
|-----------|-------|---------|
| **Impact** | â˜…â˜†â˜† to â˜…â˜…â˜… | How much this would differentiate OmniVor from pharma-standard UX |
| **Complexity** | ğŸ”§ to ğŸ”§ğŸ”§ğŸ”§ | Implementation effort to adapt the pattern |

---

## 1. Audience Comparison â€” "Help me decide between these two things"

### Current state observations

Looking at the current UI, Audience Comparison has solid bones â€” the Overlap Analysis, Metrics Comparison table with statistical significance markers, and the distribution charts at the bottom all provide real analytical value. But the experience is *flat*. Every metric gets the same visual weight. The user has to manually scan the Difference column to figure out what actually matters. The comparison answers "what's different?" but not "so what?" or "what should I do about it?"

The Quick Presets (Tier 1 vs Tier 2, Digital vs Traditional, High vs Low Engagement) are a smart addition â€” they suggest the tool knows what comparisons matter. Push that further.

---

### 1a. NerdWallet â€” Financial Product Comparison
**Why this analogue:** NerdWallet's core UX challenge is identical to yours: help someone compare two complex options (credit cards, mortgages, banks) across many dimensions and arrive at a confident decision. The entities being compared aren't simple â€” they have tradeoffs, context-dependent value, and no single "winner."

**What to steal:**

- **The verdict banner.** NerdWallet doesn't just show you two credit cards side by side â€” it leads with a synthesized recommendation: "Best for: cash back on groceries" vs. "Best for: travel rewards." Before the user sees any data, they see a *characterization* of each option. OmniVor should open the comparison with a synthesized insight: "Priority HCPs w/ Low Email Response is your **high-volume, broad-reach** audience. TX Webinar Engagers is your **high-engagement, concentrated** audience." This takes the cognitive load off the user immediately.

- **The "Our Take" editorial layer.** NerdWallet includes human-written analysis alongside the data â€” not just "4.5 stars" but "This card stands out because..." Your Audience Comparison should include an AI-generated narrative summary between the selectors and the metrics table: "These audiences overlap by only 4% (64 HCPs), suggesting they represent distinct strategic segments. TX Webinar Engagers shows 14.9% higher conversion likelihood despite being 23Ã— smaller, making them ideal for high-touch pilot campaigns. Priority HCPs drive volume (160K Rx vs 8K) but show higher churn risk (+3.6%)." The data table becomes *evidence* for the narrative, not the primary content.

- **The "Winner" badges per dimension.** NerdWallet puts a small checkmark or "Best" badge next to the winning product in each comparison row. Your Difference column already shows directional arrows, but go further: for each metric row, visually highlight which audience "wins" on that dimension, using color or iconography. The eye should be able to scan the left edge of the table and instantly see "Audience A wins on 4 metrics, Audience B wins on 3."

- **The "Compare with a different card" persistent CTA.** NerdWallet always makes it easy to swap one option. Your dropdowns do this, but consider adding a "Try a different comparison" suggestion below the results: "You might also want to compare TX Webinar Engagers against Movable High Tier (similar size, different strategy)." Smart follow-up comparisons based on what the user just learned.

**Impact:** â˜…â˜…â˜… â€” The narrative synthesis alone would make this the most intelligent comparison tool in pharma. Nobody else generates a "verdict."
**Complexity:** ğŸ”§ğŸ”§ â€” Narrative generation requires LLM integration (your agent architecture supports this). Winner badges and verdict banners are UI additions over existing data.

---

### 1b. Progressive "Compare Quotes" â€” Insurance Shopping
**Why this analogue:** Progressive built its brand on showing you competitors' prices *alongside its own*. The radical transparency of "here's how we stack up, and sometimes we lose" created trust. The comparison experience is designed to build confidence in a decision, not just present data.

**What to steal:**

- **The "you could save" framing.** Progressive always translates raw numbers into personal impact: not "$847/year" but "You could save $234." Your Difference column shows "+6.9" or "+3.6" but doesn't translate that into business impact. Instead of "Avg Engagement Score: 50 vs 56.9, Difference: +6.9", frame it as: "TX Webinar Engagers scores **14% higher** on engagement. If applied to a 1,000-HCP campaign, this could translate to ~69 additional engaged providers." Make the difference *tangible*.

- **The "Why is this different?" explainability.** Progressive explains why prices vary: age, driving record, coverage level. When two audiences differ significantly on a metric, offer an explanation: "Conversion Likelihood is 14.9 points higher for TX Webinar Engagers, likely driven by their concentration in Tier 1 (48% vs 31%) and higher webinar channel affinity." Don't just show the difference â€” explain it.

- **The emphasis on what matters to *you*.** Progressive highlights the coverage features you prioritized. Let users set "what matters most" before comparing: if they care about conversion, weight that comparison. If they care about reach, weight Rx volume. A simple "What's your priority?" toggle (Reach | Engagement | Conversion | Efficiency) that reorders and emphasizes metrics accordingly.

- **The "customize and re-quote" loop.** After seeing results, Progressive lets you adjust parameters and re-compare. After seeing that Audience A has higher churn risk, let the user say "What if I remove Tier 3 from Audience A?" and see the comparison update dynamically. Inline audience refinement within the comparison view.

**Impact:** â˜…â˜…â˜… â€” Impact translation (from abstract scores to business outcomes) is the single most-requested feature in pharma analytics. Explainability builds trust.
**Complexity:** ğŸ”§ğŸ”§ â€” Impact translation requires business multipliers (engagement â†’ Rx â†’ revenue); explainability requires correlation analysis or LLM narrative.

---

### 1c. Google Flights â€” Tradeoff Visualization
**Why this analogue:** Google Flights nails a specific pattern that Audience Comparison needs: showing tradeoffs visually. The "price vs. duration" scatter plot, the calendar heatmap, the "Best flights" vs "Cheapest" vs "Fastest" tabs â€” all designed to make tradeoffs *visible*, not hidden in numbers.

**What to steal:**

- **The tradeoff scatter plot.** Plot the two audiences on a 2Ã—2 matrix where the axes are the user's two most important metrics. Example: X-axis = Engagement Score, Y-axis = Conversion Likelihood. Each dot is an HCP in one of the two audiences, color-coded. Instantly reveals: "These audiences occupy different strategic quadrants." This is far more insightful than a table row showing averages.

- **The "Best" / "Cheapest" / "Fastest" tabs.** Google Flights pre-answers three strategic questions. OmniVor Audience Comparison could offer: **"Best for Reach"** | **"Best for Engagement"** | **"Best for ROI"** â€” each tab highlights which audience wins under that strategic lens and why.

- **The "track prices" notification.** Google Flights lets you watch a flight. After comparing, let users "watch" an audience: "Notify me if Movable High Tier's engagement score changes by more than 5 points." This turns a one-time comparison into ongoing intelligence.

**Impact:** â˜…â˜…â˜† â€” The scatter plot is a genuine "wow" visualization. Strategic lens tabs are practical and differentiating.
**Complexity:** ğŸ”§ğŸ”§ â€” Scatter plot requires per-HCP data rendering (you have this data); strategic tabs are a UI layer over existing metric calculations.

---

## 2. Action Queue â€” "What should I do next, and why?"

### Current state observations

The Action Queue screenshot reveals a significant UX problem: **visual monotony.** Every row shows 90% confidence, "High" urgency, "Expand" action, "opportunity" health. When everything is high priority, nothing is. The user's eye has nothing to grab onto â€” no hierarchy, no "start here" signal. The Reasoning column is truncated to the point of uselessness ("High affinity (score: 80) with limited engagement (1 touches). Significan..."). 

The raw materials are excellent â€” confidence, urgency, channel, reasoning â€” but the presentation treats every recommendation as equal, which undermines the entire point of a prioritized queue.

---

### 2a. Things 3 â€” The Premium To-Do List
**Why this analogue:** Things 3 (by Cultured Code) is widely considered the best-designed task manager ever built. It takes a fundamentally boring concept â€” a list of things to do â€” and makes it feel calm, intentional, and pleasurable. The secret is aggressive visual hierarchy and spatial grouping.

**What to steal:**

- **The "Today" / "Upcoming" / "Anytime" / "Someday" buckets.** Things 3 doesn't show you everything at once. It temporally segments your tasks so you only see what's relevant *right now*. The Action Queue should segment recommendations into: **"Act Now"** (high urgency + high confidence), **"This Week"** (medium urgency or needs review), **"Backlog"** (low urgency, exploratory). The current flat list of 1,610 actions is paralyzing. Show 10â€“15 in "Act Now" and let the user pull from the deeper buckets when ready.

- **The spatial breathing room.** Things 3 uses generous padding, subtle section dividers, and whitespace to make a task list feel *manageable*. The current Action Queue is a dense table. Consider: for the "Act Now" bucket, switch from table rows to cards â€” each recommendation gets vertical space, the full reasoning text visible, a clear approve/reject/defer interaction. Tables are for scanning; cards are for deciding.

- **The project grouping.** Things 3 lets you view tasks by project or by tag. The Action Queue should let users group by: **Channel** (all email recommendations together), **HCP Tier** (all Tier 1 first), **Action Type** (all "Expand" vs "Defend" vs "Reactivate"), or **Therapeutic Area**. Different groupings serve different workflows â€” a field rep groups by HCP, a brand manager groups by channel.

- **The completion animation.** Things 3 has a deeply satisfying checkmark animation when you complete a task. Small, but it creates the "clearing the inbox" psychology. When a user approves or rejects a recommendation, give them a micro-animation â€” the row slides away, the counter updates, progress feels tangible.

**Impact:** â˜…â˜…â˜… â€” Temporal bucketing alone would fix the biggest problem with the current UI. The shift from "1,610 actions" to "12 actions for today" changes the psychological relationship with the queue.
**Complexity:** ğŸ”§ğŸ”§ â€” Bucketing requires urgency/confidence threshold logic (you already have the scores); card view is a new component but straightforward.

---

### 2b. The NYT Mini Crossword / Wordle â€” The Puzzle Pattern
**Why this analogue:** This is the outside-the-box direction you hinted at. Each Action Queue record is fundamentally a micro-puzzle: here's an HCP, here's what the data says, here's what we recommend â€” *do you agree?* The cognitive activity isn't passive review; it's active judgment. Puzzle games are designed to make that kind of focused judgment feel satisfying.

**What to steal:**

- **The single-focus "board" view.** Wordle shows you one puzzle. You solve it. You move on. For complex recommendations (low confidence, high stakes), offer a "Focus Mode" that shows one recommendation as a full-screen card: the HCP's profile summary on the left, the recommendation rationale in the center, supporting evidence (channel health, competitive pressure, saturation data) on the right. All the clues, one decision. This is where the "short puzzle" metaphor lives â€” the user is synthesizing evidence from multiple modules into a judgment call.

- **The streak / daily challenge framing.** Wordle gives you one puzzle per day and tracks your streak. What if the Action Queue surfaced a "Daily Brief" â€” your top 5 highest-impact recommendations for today, framed as a focused review session? "You have 5 critical decisions today. Average review time: 3 minutes." This turns queue management from an open-ended chore into a bounded, completable daily habit. Track the streak: "You've reviewed your Daily Brief 14 days in a row."

- **The "hard mode" toggle (show your work).** Wordle's Hard Mode forces you to use confirmed letters. The Action Queue equivalent: a mode where the system shows the recommendation *without* telling you the suggested action, and you have to decide first. Then it reveals the NBO recommendation and shows whether you agreed. "You agreed with the system 78% of the time. When you disagreed, your alternative was better 31% of the time." This is both a training tool and a trust-building mechanism â€” it teaches users how the system thinks while respecting their judgment.

- **The post-puzzle debrief.** After solving Wordle, you see your guess distribution. After clearing the Daily Brief, show a summary: "Today you approved 4 actions and deferred 1. Approved actions target 3 Tier 1 HCPs across 2 channels. Estimated engagement impact: +12 points." This gives closure and quantifies the session's value.

**Impact:** â˜…â˜…â˜… â€” The Daily Brief alone transforms the Action Queue from an overwhelming log into a compelling daily ritual. The "hard mode" pattern is a genuinely novel idea for pharma.
**Complexity:** ğŸ”§ğŸ”§ â€” Daily Brief is a filtered view with threshold logic; Focus Mode is a new card component; "hard mode" requires hiding/revealing recommendation data, plus tracking agreement metrics.

---

### 2c. Linear â€” Engineering Task Triage
**Why this analogue:** Linear is the tool that proved enterprise task management doesn't have to feel like enterprise software. It's fast, keyboard-driven, and treats task triage as a core workflow deserving first-class UX â€” not an afterthought bolted onto a database.

**What to steal:**

- **The keyboard-first everything.** Linear users never touch a mouse for triage. `â†‘â†“` to navigate, `1/2/3` to set priority, `A` to assign, `Enter` to open detail. The Action Queue should support: `â†‘â†“` to navigate rows, `A` to approve, `R` to reject, `D` to defer, `Space` to expand details, `Enter` to open HCP profile. Show a subtle keyboard hint bar at the bottom: "A Approve Â· R Reject Â· D Defer Â· Space Details Â· â†‘â†“ Navigate". This makes the Action Queue *fast* in a way that no pharma tool has ever been.

- **The views system.** Linear lets you save filtered views: "My Issues," "High Priority," "Backend Team." The Action Queue should let users create and save views: "My Tier 1 Actions," "Email Channel Only," "Deferred for Review." Saved views persist across sessions and become each user's personalized workflow.

- **The sub-grouping with counts.** Linear shows issues grouped by status with counts: "In Progress (7) Â· Todo (12) Â· Backlog (34)." The Action Queue grouped by channel with counts: "ğŸ“§ Email (423) Â· ğŸ¥ Webinar (312) Â· ğŸ“ Phone (289) Â· ğŸ–¥ï¸ Digital (196) Â· ğŸ¤ Rep Visit (145) Â· ğŸ“‹ Conference (90)." Each group is collapsible. The user sees the shape of the queue before diving in.

- **The "triage" as a named workflow.** Linear has an explicit Triage view. Make "Triage" a first-class mode in the Action Queue â€” not just the default table, but a named, designed experience: "Enter Triage Mode" â†’ keyboard shortcuts activate, queue narrows to unreviewed items, progress bar appears. When triage is done: "Queue clear. 47 actions approved, 12 deferred, 3 rejected."

**Impact:** â˜…â˜…â˜… â€” Keyboard shortcuts and saved views are the difference between a tool people use weekly and a tool they use daily. "Triage Mode" gives the activity a name and a beginning/end.
**Complexity:** ğŸ”§ğŸ”§ â€” Keyboard handlers and saved views are well-understood patterns. Triage mode is a filtered view with UI state management.

---

## 3. HCP Explorer â€” "Browse, evaluate, get curious"

### Current state observations (from Audience Builder screenshot, which shows the HCP card pattern)

The HCP cards in the Live Preview show: name, specialty, institution, location, tier badge, segment badge, and engagement score. It's functional but every card looks the same â€” same layout, same visual weight, same information density. There's nothing to make one HCP "pop" over another. The user has to read each card's metadata to evaluate it. Nothing is scannable at a glance.

---

### 3a. Airbnb â€” The Listing Card That Sells
**Why this analogue:** Airbnb's listing card is one of the most studied UI patterns in consumer tech. It takes a complex entity (a property with dozens of attributes) and reduces it to a card that communicates value in under 2 seconds. The card doesn't try to show everything â€” it shows enough to earn the click.

**What to steal:**

- **The hero image as instant identity.** Every Airbnb card leads with a photo â€” it's the single biggest differentiator between listings. HCPs don't have photos (and shouldn't), but the principle applies: lead with the most *differentiating* visual element, not the most *informational*. For HCPs, this could be a compact sparkline showing their engagement trend over the last 6 months â€” rising, stable, declining. A rising sparkline immediately tells you "this HCP is getting more engaged" without reading a single number. It becomes the visual "hero" of the card.

- **The badge system for quick qualification.** Airbnb uses "Superhost," "Guest Favorite," "New" badges to let you pre-filter visually before reading details. OmniVor already has Tier and Segment badges â€” push further with *behavioral* badges: "ğŸ”¥ Rising Engagement," "âš ï¸ At Risk," "ğŸ¯ High Converter," "ğŸ†• New to Territory." These badges should be the primary visual differentiator between cards â€” not the text content.

- **The "rare find" / urgency signals.** Airbnb adds "This is a rare find" to high-demand listings. For HCPs, surface competitive intelligence at the card level: "âš¡ Competitor activity detected" or "ğŸ“ˆ 3 other brands targeting." This gives the card strategic context that no static profile data provides. It creates urgency.

- **The review score as social proof.** Airbnb's star rating is immediate, trusted, and comparable. The engagement score (53, 40, 56 in your screenshot) serves this purpose, but it's just a number. Add a micro-visual: a small radial gauge, a color-coded bar, or a letter grade (A/B/C/D) alongside the number. Make the score *feel* like a rating, not a data point. "56 Engagement" vs "A- (56)" â€” the letter grade is instantly comparable.

- **The map integration.** Airbnb always shows where the listing is. For field reps especially, an HCP's geographic context matters: "This HCP is in your territory." "This HCP is 12 miles from the one you visited yesterday." A small map marker or distance indicator on the card connects the digital profile to the physical world of field engagement.

- **The "Save to Wishlist" gesture.** Airbnb's heart icon lets you save a listing with one tap, organized into named lists. This is the Spotify "Add to Playlist" pattern applied to browsing: every HCP card should have a quick-save action that adds the HCP to an audience without leaving the Explorer view. One tap, choose the audience, done.

**Impact:** â˜…â˜…â˜… â€” The sparkline hero and behavioral badges would transform the card from "data display" to "intelligence at a glance." Field reps and brand managers would immediately understand which HCPs deserve attention.
**Complexity:** ğŸ”§ğŸ”§ â€” Sparklines require engagement history data (you have this); badges require threshold logic against existing metrics; map integration is the most complex addition.

---

### 3b. VRBO â€” The Property Card with "Why This One" Logic
**Why this analogue:** VRBO (and its recent redesigns) adds a layer that Airbnb doesn't emphasize as much: *match reasoning*. VRBO increasingly tells you *why* a listing was shown: "Great for families," "Perfect for your dates," "Top rated for cleanliness." It's not just browsing â€” it's guided browsing.

**What to steal:**

- **The "Why am I seeing this?" one-liner.** Every HCP card in search results or audience previews should include a one-line match reason when context allows: "Matched: Tier 1 Oncologist with declining email engagement" or "Similar to: Dr. Chen in your 'High Converters' audience." This turns the Explorer from a database browser into an intelligent recommendation surface. It answers the question before the user asks it.

- **The "Match %" concept.** VRBO sometimes shows a match score relative to your search criteria. When the user has active filters or NLQ criteria, show a match percentage on each card: "92% match to your query." This helps users understand *why* some results are more relevant than others, especially when NLQ parsing produces fuzzy results.

- **The "Compare" checkbox pattern.** VRBO lets you check 2-3 listings and compare them side by side. This bridge between Explorer and Audience Comparison is already partially built (multi-select â†’ compare), but make it more intentional: a persistent "Compare" tray at the bottom of the screen that accumulates selected HCPs, with a "Compare These" button that launches a mini-comparison view without leaving the Explorer.

- **The "Similar Properties" / "People Also Viewed" section.** When viewing an HCP's expanded profile (the drawer), show: "Similar HCPs" (same specialty, tier, and segment) and "Often grouped with" (HCPs that frequently appear in the same audiences). This creates an exploration loop â€” view one HCP, discover three more, add to audience, keep going.

**Impact:** â˜…â˜…â˜† â€” Match reasoning is subtle but powerful for building trust in NLQ results and filtered views. The compare tray improves multi-select workflow significantly.
**Complexity:** ğŸ”§ to ğŸ”§ğŸ”§ â€” Match reasoning requires passing filter context to the card renderer; "Similar HCPs" requires a similarity query (could be simple dimension matching or more sophisticated embedding-based similarity).

---

### 3c. Hinge (Dating App) â€” The Profile Card That Invites Opinion
**Why this analogue:** Yes, another dating app â€” but for a different reason than Tinder. Hinge's innovation is the "Most Compatible" feature and the way profile cards invite specific interaction. You don't just swipe; you *like a specific thing* about the profile (a photo, a prompt answer). This specificity creates higher-quality engagement.

**What to steal:**

- **The "Most Compatible" daily suggestion.** Hinge surfaces one profile per day that its algorithm thinks is your best match. The HCP Explorer could surface a "Recommended for You" section at the top: "Based on your recent audience building and simulation patterns, these 3 HCPs deserve your attention today." This transforms the Explorer from a passive search tool into a proactive recommendation surface. It answers the marketer's unstated question: "Where should I focus?"

- **The "like a specific thing" interaction.** On Hinge, you don't just approve someone â€” you respond to a specific prompt or photo. When a user adds an HCP to an audience from the Explorer, let them tag *why*: "Added for: high conversion potential" or "Added for: KOL influence." These micro-tags accumulate and become audience metadata â€” you can later filter the audience by "why they were added." It makes audiences self-documenting.

- **The "Standouts" premium tier.** Hinge shows you "Standouts" â€” profiles that are especially popular or compatible â€” as a premium feature. In the Explorer, visually distinguish "Standout" HCPs â€” those with unusual metric combinations that make them strategically interesting: a Tier 3 with rising engagement (someone getting more engaged despite low historical priority), or a Tier 1 with a sudden competitive pressure spike. These outliers are where the strategic gold lives, and they should visually pop in the card grid.

**Impact:** â˜…â˜…â˜† â€” "Recommended for You" and Standout flagging add genuine intelligence to browsing. The micro-tagging creates self-documenting audiences.
**Complexity:** ğŸ”§ğŸ”§ â€” Recommendation logic requires user behavior tracking (which audiences they build, which simulations they run); Standout detection requires anomaly scoring against HCP metrics.

---

## Summary: New Analogues Added

| Module | Analogue | Key Steal | Impact | Complexity |
|--------|----------|-----------|--------|------------|
| **Audience Comparison** | NerdWallet | Verdict banner + AI narrative synthesis | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| | Progressive | Impact translation + explainability per metric | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| | Google Flights | Tradeoff scatter plot + strategic lens tabs | â˜…â˜…â˜† | ğŸ”§ğŸ”§ |
| **Action Queue** | Things 3 | Temporal buckets (Act Now / This Week / Backlog) + cards for decisions | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| | NYT Mini / Wordle | Daily Brief ritual + Focus Mode puzzle + "Hard Mode" training | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| | Linear | Keyboard triage + saved views + named Triage Mode | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| **HCP Explorer** | Airbnb | Sparkline hero + behavioral badges + quick-save to audience | â˜…â˜…â˜… | ğŸ”§ğŸ”§ |
| | VRBO | "Why am I seeing this?" match reasoning + compare tray | â˜…â˜…â˜† | ğŸ”§ to ğŸ”§ğŸ”§ |
| | Hinge | "Recommended for You" daily + micro-tag on add + Standout detection | â˜…â˜…â˜† | ğŸ”§ğŸ”§ |

---

## Cross-Cutting Observation: The Uniformity Problem

Looking at the Action Queue screenshot, the deepest issue isn't the table layout â€” it's that **the data itself looks uniform**. Every row is 90% confidence, High urgency, Expand, Opportunity. This creates a "wall of sameness" that no UI pattern can fully rescue.

Before investing in UI analogues, consider whether the NBO engine's scoring needs recalibration so that recommendations have more genuine variance. A well-tuned engine should produce a power-law distribution: a few truly critical recommendations, a medium bucket of important-but-not-urgent ones, and a long tail of nice-to-haves. If the engine produces that distribution, then Things 3 / Linear / Wordle patterns can *visualize* the distribution beautifully. If the engine produces 1,610 identical-looking recommendations, even the best UI will feel flat.

The UI investment and the engine calibration should happen in parallel.

---

## Top Priority Steals from This Supplement

1. **NerdWallet â†’ Audience Comparison**: AI-generated verdict banner and narrative synthesis (your agent architecture already supports this)
2. **Things 3 â†’ Action Queue**: Temporal bucketing into "Act Now" / "This Week" / "Backlog" (requires only threshold logic on existing scores)
3. **Airbnb â†’ HCP Explorer**: Engagement sparkline on every card + behavioral badges (data already exists, just needs card redesign)
4. **Wordle â†’ Action Queue**: "Daily Brief" â€” your top 5 decisions for today, framed as a completable session
5. **Progressive â†’ Audience Comparison**: Translate abstract score differences into business impact ("This could mean ~69 more engaged providers")
